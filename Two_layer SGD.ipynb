{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义神经网络必备的函数\n",
    "import numpy as np\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    return x/np.sum(x)\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-4\n",
    "    return -np.sum(t*np.log(y+delta)) #np.log就是ln\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x): #求微分的全过程\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) #初始化，生成和x形状相同的0数组\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 还原值\n",
    "        \n",
    "    return grad\n",
    "'''\n",
    "Attention! _numerical_gradient_no_batch(f,x)只能用于一维数组,理由如下\n",
    "X=np.array([[9,8,7],[6,5,4]])\n",
    "y=np.array([1,2,3])\n",
    "X[1] #array([6,5,4])\n",
    "y[1] #2\n",
    "'''\n",
    "def numerical_gradient(f, X): #求偏导数,即梯度\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X): #enumerate实例见enumerate_example.py\n",
    "            #enumerate的作用是给x标序号，存在idx中，默认从0开始\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "            #grad得先定义成一个与X同大小的零矩阵\n",
    "        return grad\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.1, step_num=100): #梯度下降\n",
    "    x=init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= grad*lr\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:\n",
      "[[-0.5488577   1.42385074 -0.64955049]\n",
      " [ 1.1386192  -0.58190248 -0.19177872]]\n",
      "p:\n",
      "[0.41385821 0.36096311 0.22517869]\n",
      "maximum index:\n",
      "0\n",
      "loss:\n",
      "0.8819902551103512\n"
     ]
    }
   ],
   "source": [
    "#简单地构建一个神经网络\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #正态分布初始化\n",
    "        \n",
    "    def predict(self, x):\n",
    "        a1 = np.dot(x, self.W)\n",
    "        z1 = sigmoid(a1)\n",
    "        y = softmax(z1)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    \n",
    "#简单应用\n",
    "net = simpleNet()\n",
    "print('W:')\n",
    "print(net.W)\n",
    "x = np.array([0.6,0.9])\n",
    "p = net.predict(x)\n",
    "print('p:')\n",
    "print(p)\n",
    "label = np.argmax(p)\n",
    "print('maximum index:')\n",
    "print(label)\n",
    "t = np.array([1,0,0])\n",
    "loss = net.loss(x, t)\n",
    "print('loss:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造一个完整的两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Two_layer_Net:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.param = {}\n",
    "        self.param['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n",
    "        self.param['b1'] = np.zeros_like(hidden_size)\n",
    "        self.param['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std\n",
    "        self.param['b2'] = np.zeros_like(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.param['W1'], self.param['W2']\n",
    "        b1, b2 = self.param['b1'], self.param['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        ay = np.argmax(y, axis=1)\n",
    "        at = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(ay==at)/float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t): #目标函数就是损失函数\n",
    "        loss_W = lambda W:self.loss(x, t)\n",
    "        \n",
    "        grad = {}\n",
    "        grad['W1'] = numerical_gradient(loss_W, self.param['W1'])\n",
    "        grad['b1'] = numerical_gradient(loss_W, self.param['b1'])\n",
    "        grad['W2'] = numerical_gradient(loss_W, self.param['W2'])\n",
    "        grad['b2'] = numerical_gradient(loss_W, self.param['b2'])\n",
    "        \n",
    "        return grad\n",
    "''''\n",
    "    def gradient_descent(self, x, t, lr, step_num):\n",
    "        grad = self.numerical_gradient(x, t)\n",
    "        for i in range(step_num):\n",
    "            for key in ('W1','b1','W2','b2'):\n",
    "                self.param[key] -= lr*grad[key]\n",
    "        return param\n",
    "''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.普通梯度下降（有问题，得改）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "from dataset.mnist import load_mnist\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = Two_layer_Net(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iter_num = 1000\n",
    "batch_size = 100\n",
    "lr = 0.1\n",
    "Total_num = x_train.shape[0]\n",
    "\n",
    "train_loss_list = []\n",
    "for i in range(iter_num):\n",
    "    batch_chosen = np.random.choice(Total_num, batch_size, replace=False) #replace=False表示不放回\n",
    "    x_batch = x_train[batch_chosen]\n",
    "    t_batch = t_train[batch_chosen]\n",
    "    \n",
    "    #y = network.predict(x_batch)\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    for i in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.param[i] -= lr*grad[i]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "train_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.用反向误差传播法的梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../common/functions.py:59: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  -np.sum(np.log(np.array(y) + 1e-7)) / y_size\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 绘制图形\\nmarkers = {\\'train\\': \\'o\\', \\'test\\': \\'s\\'}\\nx = np.arange(len(train_acc_list))\\nplt.plot(x, train_acc_list, label=\\'train acc\\')\\nplt.plot(x, test_acc_list, label=\\'test acc\\', linestyle=\\'--\\')\\nplt.xlabel(\"epochs\")\\nplt.ylabel(\"accuracy\")\\nplt.ylim(0, 1.0)\\nplt.legend(loc=\\'lower right\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 4000  # 适当设定循环的次数\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) #这个用的是梯度法\n",
    "    grad = network.gradient(x_batch, t_batch) #这个用的是误差反向传播法\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key] #迭代，变化参数数值\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "#绘制损失函数随着迭代次数的变化图\n",
    "plt.plot(range(1,iters_num+1),train_loss_list)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
